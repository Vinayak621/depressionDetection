{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "efed345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e31dd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Sheet_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0020ab23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>response_1</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>I try and avoid this sort of conflict</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>response_2</td>\n",
       "      <td>flagged</td>\n",
       "      <td>Had a friend open up to me about his mental ad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>response_3</td>\n",
       "      <td>flagged</td>\n",
       "      <td>I saved a girl from suicide once. She was goin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>response_4</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>i cant think of one really...i think i may hav...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>response_5</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Only really one friend who doesn't fit into th...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>response_76</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Now that I've been through it, although i'm no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>response_77</td>\n",
       "      <td>flagged</td>\n",
       "      <td>when my best friends mom past away from od'ing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>response_78</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>As a camp counselor I provide stability in kid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>response_79</td>\n",
       "      <td>flagged</td>\n",
       "      <td>My now girlfriend used to have serious addicti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>response_80</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>The one person I ever talked to it was because...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    response_id        class  \\\n",
       "0    response_1  not_flagged   \n",
       "1    response_2      flagged   \n",
       "2    response_3      flagged   \n",
       "3    response_4  not_flagged   \n",
       "4    response_5  not_flagged   \n",
       "..          ...          ...   \n",
       "75  response_76  not_flagged   \n",
       "76  response_77      flagged   \n",
       "77  response_78  not_flagged   \n",
       "78  response_79      flagged   \n",
       "79  response_80  not_flagged   \n",
       "\n",
       "                                        response_text Unnamed: 3  Unnamed: 4  \\\n",
       "0               I try and avoid this sort of conflict        NaN         NaN   \n",
       "1   Had a friend open up to me about his mental ad...        NaN         NaN   \n",
       "2   I saved a girl from suicide once. She was goin...        NaN         NaN   \n",
       "3   i cant think of one really...i think i may hav...        NaN         NaN   \n",
       "4   Only really one friend who doesn't fit into th...                    NaN   \n",
       "..                                                ...        ...         ...   \n",
       "75  Now that I've been through it, although i'm no...        NaN         NaN   \n",
       "76  when my best friends mom past away from od'ing...        NaN         NaN   \n",
       "77  As a camp counselor I provide stability in kid...        NaN         NaN   \n",
       "78  My now girlfriend used to have serious addicti...        NaN         NaN   \n",
       "79  The one person I ever talked to it was because...        NaN         NaN   \n",
       "\n",
       "   Unnamed: 5  Unnamed: 6 Unnamed: 7  \n",
       "0         NaN         NaN        NaN  \n",
       "1         NaN         NaN        NaN  \n",
       "2         NaN         NaN        NaN  \n",
       "3         NaN         NaN        NaN  \n",
       "4         NaN         NaN        NaN  \n",
       "..        ...         ...        ...  \n",
       "75        NaN         NaN        NaN  \n",
       "76        NaN         NaN        NaN  \n",
       "77        NaN         NaN        NaN  \n",
       "78        NaN         NaN        NaN  \n",
       "79        NaN         NaN        NaN  \n",
       "\n",
       "[80 rows x 8 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "067d1deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a52a3813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80 entries, 0 to 79\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   response_id    80 non-null     object \n",
      " 1   class          80 non-null     object \n",
      " 2   response_text  80 non-null     object \n",
      " 3   Unnamed: 3     2 non-null      object \n",
      " 4   Unnamed: 4     0 non-null      float64\n",
      " 5   Unnamed: 5     1 non-null      object \n",
      " 6   Unnamed: 6     0 non-null      float64\n",
      " 7   Unnamed: 7     1 non-null      object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 5.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30767878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response_id       0\n",
       "class             0\n",
       "response_text     0\n",
       "Unnamed: 3       78\n",
       "Unnamed: 4       80\n",
       "Unnamed: 5       79\n",
       "Unnamed: 6       80\n",
       "Unnamed: 7       79\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "81d3539a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5fd2d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['response_id','class','response_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7ced2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3ecb39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I try and avoid this sort of conflict'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['response_text'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0eb1035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VINAYAKA\\AppData\\Local\\Temp\\ipykernel_7964\\3842708470.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['response_text']=data['response_text'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "data['response_text']=data['response_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b33bbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f4a0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    patt=re.compile('<.*?>')\n",
    "    return patt.sub('',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6bda0312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 i try and avoid this sort of conflict\n",
       "1     had a friend open up to me about his mental ad...\n",
       "2     i saved a girl from suicide once. she was goin...\n",
       "3     i cant think of one really...i think i may hav...\n",
       "4     only really one friend who doesn't fit into th...\n",
       "                            ...                        \n",
       "75    now that i've been through it, although i'm no...\n",
       "76    when my best friends mom past away from od'ing...\n",
       "77    as a camp counselor i provide stability in kid...\n",
       "78    my now girlfriend used to have serious addicti...\n",
       "79    the one person i ever talked to it was because...\n",
       "Name: response_text, Length: 80, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['response_text'].apply(remove_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "656a8aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No URL tag: i try and avoid this sort of conflict\n",
      "No URL tag: had a friend open up to me about his mental addiction to weed and how it was taking over his life and making him depressed\n",
      "No URL tag: i saved a girl from suicide once. she was going to swallow a bunch of pills and i talked her out of it in a very calm, loving way.\n",
      "No URL tag: i cant think of one really...i think i may have indirectly\n",
      "No URL tag: only really one friend who doesn't fit into the any of the above categories. her therapist calls it spiraling.\" anyway she pretty much calls me any time she is frustrated by something with  her boyfriend to ask me if it's logical or not. before they would just fight and he would call her crazy. now she asks me if it's ok he didn't say \"please\" when he said  \"hand me the remote.\"\n",
      "No URL tag: a couple of years ago my friends was going to switch school because of low self esteem too. i helped him overcome that shit too\n",
      "No URL tag: roommate when he was going through death and loss of a gf. did anything to get him out of his bedroom.\n",
      "No URL tag: i've had a couple of friends (you could say more than friends) with quite severe depression/ emotional problems. i helped for a while but eventually both relationships started to suffer as a result of both our personal problems\n",
      "No URL tag: listened to someone talk about relationship troubles. offered some advice from personal experience.\n",
      "No URL tag: i will always listen. i comforted my sister when she lost her virgity the same night she walked in on her boyfriend cutting himself, and then our parents found out she threw a house part. simply bring supportive was my focus.\n",
      "No URL tag: took a week off work, packed up the car and picked up a friend who was on the verge of losing it and went camping/surfing for a week. his parents were a big part of the problem and being away from them and others and physical activity every day for a week. but more just being around helped i feel.\n",
      "No URL tag: on the memorial anniversary of my friends father i was with him to give support and remind him that he's not alone.\n",
      "No URL tag: anxious girlfriend always needs my help\n",
      "No URL tag: never\n",
      "No URL tag: you as a mom\n",
      "No URL tag: ex gf was a cutter/suicidal, got her out of her own issues while slowly dying inside.\n",
      "No URL tag: i have helped advise friends who have faced circumstances similar to mine\n",
      "No URL tag: i've helped friends out before\n",
      "No URL tag: a friend that is a girl and just talk to her tryin to make her feel better\n",
      "No URL tag: expressing concern and openness to friends when they are dealing with troubles. letting them know they can fell open for dialog.\n",
      "No URL tag: listening to girlfriend's problems\n",
      "No URL tag: friend was thinking about suicide, after a few days i managed to talk her down.\n",
      "No URL tag: having gone through depression and anxiety myself, i understand the struggles and have a few personal methods to cope when stuff hits. having this knowledge has allowed me to help several people on the internet as well as my other friends when they have faced similar issues and talk to me about them. i understand how important listening is and offer my experiences to help them get through what they face.\n",
      "No URL tag: sometimes my friends bring up issues they are having with themselves, and i feel that i give them good advice and make them feel adequate and normal. i just try to be kind and helpful and honest.\n",
      "No URL tag: never i guess people dont see me as a guy they would go to\n",
      "No URL tag: last year, my best friend was diagnosed with anxiety disorder and depression. she tried to commit suicide. i tried my best to be there for her the entire time throughout her recovery.\n",
      "No URL tag: cleaning up my friend's campsite and slightly helping him while he was living all summer as an alcoholic in the mt. hood woods.\n",
      "No URL tag: i listen pretty damn well. better at that as apposed to telling my own story.\n",
      "No URL tag: helping a friend through dealing with his alcoholic mother.\n",
      "No URL tag: i used to tutor homeless men at a shelter to help them obtain their ged's. they were all age 50+ and some of them were even reading at a first grade level.\n",
      "No URL tag: i am a high school teacher so almost everyday.\n",
      "No URL tag: friends often come to me to talk about the issues that they're facing in life.\n",
      "No URL tag: summer camp, countless kids have the same issue. i tell them i how it is when you cant listen or are restless.\n",
      "No URL tag: haha. in eight grade, kid got rejected twice. wanted her to sign his yearbook but he was too nervous. i told him she doesn't really care about rejecting him and she thinks no less of him. he did it.    probably more, but that's what comes up off the top of my head.\n",
      "No URL tag: don't have a specific example but just letting people know you're there if they want to talk.\n",
      "No URL tag: some of my friends have gone through the same type of struggle i have been through. i let them know that they could talk to me about anything and that being honest with yourself even if the truth hurts is the best way to start to get back up.\n",
      "No URL tag: friend who had big addiction issues, ended up being completely isolated, skipped school, and had a very low self esteem. i convinced him to go see a doc together, and promised him that i would go through everything with him. he went to rehab for a few months and now he's clean. but the complete lack of support from his family, from school, changed him. i think it was too late when i helped him (he was like that for 5 months) and he'll never be the same as before.\n",
      "No URL tag: i've always been a good listener for people and i think a lot of people just want to be heard and not necessarily fixed. they just need to express themselves to relief their thoughts to others so they're not trapped so much within the confines of their own head.  i've always lent an ear for someone to speak to.\n",
      "No URL tag: one of my best friends was diagnosed with cancer and i did as much as i could for him during that time. i visited him almost every day when he was in treatment just so he didn't have to be alone or anything. when he came back to school i helped him get caught up with school work too.\n",
      "No URL tag: i don't think i'm qualified to be a serious resource for someone else.\n",
      "No URL tag: many times for my one friend who tried hard drugs (cocaine) and was dealing with depression because of girls\n",
      "No URL tag: a lot for my friends. when they have had girl problems, or have just been down in the dumps about life. i always try to look on the positive side of life, and i try to reflect that on my friends' lives as well.\n",
      "No URL tag: i'm the theripist of my friends, they always come to me\n",
      "No URL tag: describe?    nah\n",
      "No URL tag: my brother went through the same struggles i did, after i found help i was able to set him on the path to healing.\n",
      "No URL tag: friend got dumped by girl he had been in a relationship in for 2 years. i stayed by his side until he stopped feeling bad about it. i was there for him.\n",
      "No URL tag: i open myself to any friend in need of emotional support.\n",
      "No URL tag: gf and i help her through a lot of shit because i myself have been through a lot of shit.\n",
      "No URL tag: i had a friend that would go off about girls and get super depressed. he told me he was going to kill himself one night and i drove 1n hour and a half to go look for him. by then the cops had already taken him to the hospital but me and a friend went and hung out with him. also been there to help try and talk him down. another person i hadnt been friends with for years cause of some stuff called me out of the blue on thankgiving last year. i called back and got a hospital number so i knew something was up. ended up being a kid who i was friends with years before but was a huge douche to me. he was in the psych ward for trying to kill himself. i got a hold of him and talked to him for a while. whenever he would call i would answer and be there even though i still had the shit from years earlier on my mind. there are people that i might not want to be around but if it's a situation like this i'll be there for people. my goal or purpose in life that's sustained my survival is to help make other peoples lives a little bit better. make people laugh when they're sad, help people out when nobody else will. basically treat other humans even strangers with the respect and kindness of a friend. that's all people need sometime. let people talk through their shit with me and offer helpful advice if i can. i'm still swimming in my own shit but that's enough to keep me going, keep that little light at the end of the tunnel switched on. idk how long that will last but i'll be here till it flickers out.\n",
      "No URL tag: sometimes i'll calm my friends down after bad stuff happens.\n",
      "No URL tag: i once acted as a resource for someone who was struggling in school, and i helped them with their schoolwork.\n",
      "No URL tag: my friend was going through a period of intense depression and i made sure that i was always there for him.\n",
      "No URL tag: i have talked though some major blows in my girlfriends life. she has had some tough times, and i helped her with them.\n",
      "No URL tag: talking to a friend over a break up.\n",
      "No URL tag: i care about other people more than myself so i listen and help them through tough times\n",
      "No URL tag: over the internet a lot of people write to me to talk about their problems. its often girl related, i dont know, i guess it can be nice to rant to someone you dont know irl/someone who can look at the situation a little bit more objective? i think a lot of guys also feel that it can be hard to talk to girl friends irl about their innermost feelings sometimes, thats the feeling i get at least. my friend jokingly refers to me as an agony aunt\"\n",
      "No URL tag: friends sometimes come up to me and ask for advice,  i try and give it the best i possibly can\n",
      "No URL tag: my friend dealt with anxiety and this desire for everything in her life to be perfect, she describes it as caring what happens to much, but either way i simply talked to her and when she would try and change the subject i would have to drag her back in due the fact that she didn't want to talk about it, but in reality she wouldn't get better unless i pulled her back into it and helped her through it. but how i helped was by basically talking to her, and giving her the advice she needed to hear, not the one she wanted to such as oh you'll be ok  this will all blow over\" what i said was more along the lines of being so blunt that many may find it rude but for her and i it was essential to making any progress.\"\"y friend dealt with anxiety and this desire for everything in her life to be perfect  she describes it as caring what happens to much  but either way i simply talked to her and when \"\n",
      "No URL tag: simply being there as a friend for somebody who's struggling.\n",
      "No URL tag: i've had some friends come to me saying people or acquaintances they've known who have killed themselves try and find comfort with me because my best friend killed himself my junior year of high school so they've come to me hoping to find some answers or peace in the turmoil.\n",
      "No URL tag: people calling, talk some sense into them, maybe facebook chat..\n",
      "No URL tag: having self harmed in the past,  i shared my experience with a friend in hopes of swaying her from those actions.\n",
      "No URL tag: have worked through problems with friends\n",
      "No URL tag: when my friend needed help when she was going through some tough stuff. i knew that she needed it but she wouldn't admit it. so i brought it up and initiated it and we talked about it and it really helped her.\n",
      "No URL tag: my sister has some pretty severe issues  with her mental health. i try to be there for her when she needs it and i'd like to think its helped her.\n",
      "No URL tag: helped friends through stuff.\n",
      "No URL tag: i helped encourage a friend to go to therapy, and often listened to their problem to let them know there was someone there for them.\n",
      "No URL tag: i haven't really met anyone dealing with attention issues. so i haven't had a chance to help unfortunately.\n",
      "No URL tag: there are too many to remember specifically, but being as helpful and rational as possible (as well as just listening) with people that need it is a fairly common occurrence\n",
      "No URL tag: one of my friends was feeling down, so he came over and talked it out while i listened and gave him some input.\n",
      "No URL tag: never.\n",
      "No URL tag: i have tried but am absolutely horrable at it and am way too blunt.\n",
      "No URL tag: ex girlfriend had depression and anxiety. i used to hold her and listen as she told me what was going on\n",
      "No URL tag: always. i don't judge people to much, and can listen well most of the time. so naturally people come to me with their problems.\n",
      "No URL tag: my grandmother went through some severe depression shortly after i had some difficulties. using my personal experience to relate with her and share what i used to get better was helpful to comfort her.\n",
      "No URL tag: now that i've been through it, although i'm not even where i'd like to be, i'm extremely open about sharing my experience with others and helping friends going through similar situations. and please if you have any other questions about my situation don't hesitate to email me. i'm an open book and excited to see how many people you're going to help.\n",
      "No URL tag: when my best friends mom past away from od'ing when he was in grade 5\n",
      "No URL tag: as a camp counselor i provide stability in kids lives who may have troubled home situations.\n",
      "No URL tag: my now girlfriend used to have serious addiction troubles before we started dating and felt as though her addiction defined her as a person. she thought that all people saw when they looked at her was the addiction. i spent many nights with her talking and letting her vent. i was one of the only people supporting her and she felt as though i could help because i had been in her spot.\n",
      "No URL tag: the one person i ever talked to it was because we were both going through the same thing. us talking together helped, it was important to realize you aren't alone\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "pattern = r'<a\\s+(?:[^>]*?\\s+)?href=[\\'\"]([^\\'\"]*)[\\'\"][^>]*>(.*?)<\\/a>'\n",
    "\n",
    "# Check if values contain URL tags\n",
    "c=0\n",
    "for value in data['response_text']:\n",
    "    if re.search(pattern, value):\n",
    "        print(f\"URL tag found: {value}\")\n",
    "    else:\n",
    "        print(f\"No URL tag: {value}\")\n",
    "        c+=1\n",
    "print(c)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde6097",
   "metadata": {},
   "source": [
    "#  No url tags in the entire dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e6681b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4913412a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f8f30677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reomove_punc(text):\n",
    "    for i in exclude:\n",
    "        text=text.replace(i,'')\n",
    "    return(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8ad3d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'people calling talk some sense into them maybe facebook chat'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=\"people calling, talk some sense into them, maybe facebook chat..\"\n",
    "reomove_punc(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "291f3448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 i try and avoid this sort of conflict\n",
       "1     had a friend open up to me about his mental ad...\n",
       "2     i saved a girl from suicide once she was going...\n",
       "3     i cant think of one reallyi think i may have i...\n",
       "4     only really one friend who doesnt fit into the...\n",
       "                            ...                        \n",
       "75    now that ive been through it although im not e...\n",
       "76    when my best friends mom past away from oding ...\n",
       "77    as a camp counselor i provide stability in kid...\n",
       "78    my now girlfriend used to have serious addicti...\n",
       "79    the one person i ever talked to it was because...\n",
       "Name: response_text, Length: 80, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['response_text'].apply(reomove_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fee3b",
   "metadata": {},
   "source": [
    "# Chat word treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8471aac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid line: QPSA?\tQue Pasa?\n",
      "{'AFAIK': 'As Far As I Know', 'AFK': 'Away From Keyboard', 'ASAP': 'As Soon As Possible', 'ATK': 'At The Keyboard', 'ATM': 'At The Moment', 'A3': 'Anytime, Anywhere, Anyplace', 'BAK': 'Back At Keyboard', 'BBL': 'Be Back Later', 'BBS': 'Be Back Soon', 'BFN': 'Bye For Now', 'B4N': 'Bye For Now', 'BRB': 'Be Right Back', 'BRT': 'Be Right There', 'BTW': 'By The Way', 'B4': 'Before', 'CU': 'See You', 'CUL8R': 'See You Later', 'CYA': 'See You', 'FAQ': 'Frequently Asked Questions', 'FC': 'Fingers Crossed', 'FWIW': \"For What It's Worth\", 'FYI': 'For Your Information', 'GAL': 'Get A Life', 'GG': 'Good Game', 'GN': 'Good Night', 'GMTA': 'Great Minds Think Alike', 'GR8': 'Great!', 'G9': 'Genius', 'IC': 'I See', 'ICQ': 'I Seek you (also a chat program)', 'ILU': 'ILU: I Love You', 'IMHO': 'In My Honest/Humble Opinion', 'IMO': 'In My Opinion', 'IOW': 'In Other Words', 'IRL': 'In Real Life', 'KISS': 'Keep It Simple, Stupid', 'LDR': 'Long Distance Relationship', 'LMAO': 'Laugh My A.. Off', 'LOL': 'Laughing Out Loud', 'LTNS': 'Long Time No See', 'L8R': 'Later', 'MTE': 'My Thoughts Exactly', 'M8': 'Mate', 'NRN': 'No Reply Necessary', 'OIC': 'Oh I See', 'PITA': 'Pain In The A..', 'PRT': 'Party', 'PRW': 'Parents Are Watching', 'ROFL': 'Rolling On The Floor Laughing', 'ROFLOL': 'Rolling On The Floor Laughing Out Loud', 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off', 'SK8': 'Skate', 'STATS': 'Your sex and age', 'ASL': 'Age, Sex, Location', 'THX': 'Thank You', 'TTFN': 'Ta-Ta For Now!', 'TTYL': 'Talk To You Later', 'U': 'You', 'U2': 'You Too', 'U4E': 'Yours For Ever', 'WB': 'Welcome Back', 'WTF': 'What The F...', 'WTG': 'Way To Go!', 'WUF': 'Where Are You From?', 'W8': 'Wait...', '7K': 'Sick:-D Laugher'}\n"
     ]
    }
   ],
   "source": [
    "abbreviations = {}\n",
    "\n",
    "with open('slang.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if '=' in line:\n",
    "            abbreviation, expansion = line.split('=', 1)\n",
    "            abbreviations[abbreviation] = expansion\n",
    "        else:\n",
    "            print(f\"Invalid line: {line}\")\n",
    "\n",
    "print(abbreviations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79f58982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFAIK': 'As Far As I Know',\n",
       " 'AFK': 'Away From Keyboard',\n",
       " 'ASAP': 'As Soon As Possible',\n",
       " 'ATK': 'At The Keyboard',\n",
       " 'ATM': 'At The Moment',\n",
       " 'A3': 'Anytime, Anywhere, Anyplace',\n",
       " 'BAK': 'Back At Keyboard',\n",
       " 'BBL': 'Be Back Later',\n",
       " 'BBS': 'Be Back Soon',\n",
       " 'BFN': 'Bye For Now',\n",
       " 'B4N': 'Bye For Now',\n",
       " 'BRB': 'Be Right Back',\n",
       " 'BRT': 'Be Right There',\n",
       " 'BTW': 'By The Way',\n",
       " 'B4': 'Before',\n",
       " 'CU': 'See You',\n",
       " 'CUL8R': 'See You Later',\n",
       " 'CYA': 'See You',\n",
       " 'FAQ': 'Frequently Asked Questions',\n",
       " 'FC': 'Fingers Crossed',\n",
       " 'FWIW': \"For What It's Worth\",\n",
       " 'FYI': 'For Your Information',\n",
       " 'GAL': 'Get A Life',\n",
       " 'GG': 'Good Game',\n",
       " 'GN': 'Good Night',\n",
       " 'GMTA': 'Great Minds Think Alike',\n",
       " 'GR8': 'Great!',\n",
       " 'G9': 'Genius',\n",
       " 'IC': 'I See',\n",
       " 'ICQ': 'I Seek you (also a chat program)',\n",
       " 'ILU': 'ILU: I Love You',\n",
       " 'IMHO': 'In My Honest/Humble Opinion',\n",
       " 'IMO': 'In My Opinion',\n",
       " 'IOW': 'In Other Words',\n",
       " 'IRL': 'In Real Life',\n",
       " 'KISS': 'Keep It Simple, Stupid',\n",
       " 'LDR': 'Long Distance Relationship',\n",
       " 'LMAO': 'Laugh My A.. Off',\n",
       " 'LOL': 'Laughing Out Loud',\n",
       " 'LTNS': 'Long Time No See',\n",
       " 'L8R': 'Later',\n",
       " 'MTE': 'My Thoughts Exactly',\n",
       " 'M8': 'Mate',\n",
       " 'NRN': 'No Reply Necessary',\n",
       " 'OIC': 'Oh I See',\n",
       " 'PITA': 'Pain In The A..',\n",
       " 'PRT': 'Party',\n",
       " 'PRW': 'Parents Are Watching',\n",
       " 'ROFL': 'Rolling On The Floor Laughing',\n",
       " 'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
       " 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
       " 'SK8': 'Skate',\n",
       " 'STATS': 'Your sex and age',\n",
       " 'ASL': 'Age, Sex, Location',\n",
       " 'THX': 'Thank You',\n",
       " 'TTFN': 'Ta-Ta For Now!',\n",
       " 'TTYL': 'Talk To You Later',\n",
       " 'U': 'You',\n",
       " 'U2': 'You Too',\n",
       " 'U4E': 'Yours For Ever',\n",
       " 'WB': 'Welcome Back',\n",
       " 'WTF': 'What The F...',\n",
       " 'WTG': 'Way To Go!',\n",
       " 'WUF': 'Where Are You From?',\n",
       " 'W8': 'Wait...',\n",
       " '7K': 'Sick:-D Laugher'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f227b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chat_words(text):\n",
    "    new_text=[]\n",
    "    for i in text.split():\n",
    "        if i.upper() in abbreviations:\n",
    "            new_text.append(abbreviations[i.upper()])\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e6c3a08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 i try and avoid this sort of conflict\n",
       "1     had a friend open up to me about his mental ad...\n",
       "2     i saved a girl from suicide once. she was goin...\n",
       "3     i cant think of one really...i think i may hav...\n",
       "4     only really one friend who doesn't fit into th...\n",
       "                            ...                        \n",
       "75    now that i've been through it, although i'm no...\n",
       "76    when my best friends mom past away from od'ing...\n",
       "77    as a camp counselor i provide stability in kid...\n",
       "78    my now girlfriend used to have serious addicti...\n",
       "79    the one person i ever talked to it was because...\n",
       "Name: response_text, Length: 80, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['response_text'].apply(remove_chat_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ae31094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 i try and avoid this sort of conflict\n",
       "1     had a friend open up to me about his mental ad...\n",
       "2     i saved a girl from suicide once. she was goin...\n",
       "3     i cant think of one really...i think i may hav...\n",
       "4     only really one friend who doesn't fit into th...\n",
       "                            ...                        \n",
       "75    now that i've been through it, although i'm no...\n",
       "76    when my best friends mom past away from od'ing...\n",
       "77    as a camp counselor i provide stability in kid...\n",
       "78    my now girlfriend used to have serious addicti...\n",
       "79    the one person i ever talked to it was because...\n",
       "Name: response_text, Length: 80, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['response_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8e5df176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d896f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_textblob_correction(value):\n",
    "    blob = TextBlob(value)\n",
    "    corrected_value = str(blob.correct())\n",
    "    return corrected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "012dab04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 i try and avoid this sort of conflict\n",
       "1     had a friend open up to me about his mental ad...\n",
       "2     i saved a girl from suicide once. she was goin...\n",
       "3     i can think of one really...i think i may have...\n",
       "4     only really one friend who doesn't fit into th...\n",
       "                            ...                        \n",
       "75    now that i've been through it, although i'm no...\n",
       "76    when my best friends mon past away from of'ing...\n",
       "77    as a camp counselor i provide stability in kis...\n",
       "78    my now girlfriend used to have serious additio...\n",
       "79    the one person i ever talked to it was because...\n",
       "Name: response_text, Length: 80, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['response_text'].apply(apply_textblob_correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbf8b2",
   "metadata": {},
   "source": [
    "# Removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "79f16d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24cd4ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "394774be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_words(text):\n",
    "    new_lis=[]\n",
    "    for word in text.split():\n",
    "        if word in stop_words:\n",
    "            new_lis.append('')\n",
    "        else:\n",
    "            new_lis.append(word)\n",
    "    x=new_lis[:]\n",
    "    new_lis.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d41e9bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VINAYAKA\\AppData\\Local\\Temp\\ipykernel_7964\\3619036168.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['response_text']=data['response_text'].apply(rem_words)\n"
     ]
    }
   ],
   "source": [
    "data['response_text']=data['response_text'].apply(rem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6635e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "\n",
    "def find_emoji_rows(column):\n",
    "    emoji_rows = []\n",
    "    for text in column:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in emoji.demojize(text)):\n",
    "            emoji_rows.append(text)\n",
    "    return emoji_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "389e7cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     []\n",
       "1     []\n",
       "2     []\n",
       "3     []\n",
       "4     []\n",
       "      ..\n",
       "75    []\n",
       "76    []\n",
       "77    []\n",
       "78    []\n",
       "79    []\n",
       "Name: response_text, Length: 80, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['response_text'].apply(find_emoji_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "362f59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ce79c807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VINAYAKA\\AppData\\Local\\Temp\\ipykernel_7964\\824410773.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['tokens'] = data['response_text'].apply(lambda x: word_tokenize(x))\n"
     ]
    }
   ],
   "source": [
    "data['tokens'] = data['response_text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0a2b1c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [try, avoid, sort, conflict]\n",
       "1     [friend, open, mental, addiction, weed, taking...\n",
       "2     [saved, girl, suicide, once, ., going, swallow...\n",
       "3     [cant, think, one, really, ..., i, think, may,...\n",
       "4     [really, one, friend, fit, categories, ., ther...\n",
       "                            ...                        \n",
       "75    [i, 've, it, ,, although, i, 'm, even, i, 'd, ...\n",
       "76    [best, friends, mom, past, away, od'ing, grade...\n",
       "77    [camp, counselor, provide, stability, kids, li...\n",
       "78    [girlfriend, used, serious, addiction, trouble...\n",
       "79    [one, person, ever, talked, going, thing, ., u...\n",
       "Name: tokens, Length: 80, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2995beeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>response_1</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>try  avoid  sort  conflict</td>\n",
       "      <td>[try, avoid, sort, conflict]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>response_2</td>\n",
       "      <td>flagged</td>\n",
       "      <td>friend open      mental addiction  weed     ...</td>\n",
       "      <td>[friend, open, mental, addiction, weed, taking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>response_3</td>\n",
       "      <td>flagged</td>\n",
       "      <td>saved  girl  suicide once.   going  swallow  ...</td>\n",
       "      <td>[saved, girl, suicide, once, ., going, swallow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>response_4</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>cant think  one really...i think  may  indire...</td>\n",
       "      <td>[cant, think, one, really, ..., i, think, may,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>response_5</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>really one friend   fit       categories.  th...</td>\n",
       "      <td>[really, one, friend, fit, categories, ., ther...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response_id        class                                      response_text  \\\n",
       "0  response_1  not_flagged                         try  avoid  sort  conflict   \n",
       "1  response_2      flagged    friend open      mental addiction  weed     ...   \n",
       "2  response_3      flagged   saved  girl  suicide once.   going  swallow  ...   \n",
       "3  response_4  not_flagged   cant think  one really...i think  may  indire...   \n",
       "4  response_5  not_flagged   really one friend   fit       categories.  th...   \n",
       "\n",
       "                                              tokens  \n",
       "0                       [try, avoid, sort, conflict]  \n",
       "1  [friend, open, mental, addiction, weed, taking...  \n",
       "2  [saved, girl, suicide, once, ., going, swallow...  \n",
       "3  [cant, think, one, really, ..., i, think, may,...  \n",
       "4  [really, one, friend, fit, categories, ., ther...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a949d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c402045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26f971f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokens1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokens1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7964\\757062905.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokens1'"
     ]
    }
   ],
   "source": [
    "data['tokens1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "435d6c3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tokens1'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7964\\3072049812.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4955\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4956\u001b[0m         \"\"\"\n\u001b[1;32m-> 4957\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4958\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4959\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4267\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4311\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4312\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6659\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6660\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6661\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6662\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6663\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['tokens1'] not found in axis\""
     ]
    }
   ],
   "source": [
    "data.drop(['tokens1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "79bdcf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VINAYAKA\\AppData\\Local\\Temp\\ipykernel_7964\\1513625789.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['tokens1'] = data['response_text'].apply(lambda x: [token.text for token in nlp(x)])\n"
     ]
    }
   ],
   "source": [
    "data['tokens1'] = data['response_text'].apply(lambda x: [token.text for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb3ffe8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [ , try,  , avoid,  , sort,  , conflict]\n",
       "1     [  , friend, open,      , mental, addiction,  ...\n",
       "2     [ , saved,  , girl,  , suicide, once, .,   , g...\n",
       "3     [ , ca, nt, think,  , one, really, ..., i, thi...\n",
       "4     [ , really, one, friend,   , fit,       , cate...\n",
       "                            ...                        \n",
       "75    [  , i, 've,   , it, ,, although, i, 'm,  , ev...\n",
       "76    [  , best, friends, mom, past, away,  , od'ing...\n",
       "77    [  , camp, counselor,  , provide, stability,  ...\n",
       "78    [  , girlfriend, used,   , serious, addiction,...\n",
       "79    [ , one, person,  , ever, talked,        , goi...\n",
       "Name: tokens1, Length: 80, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dca46119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1eda6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f7a28751",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b7e3803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    return \" \".join([pos.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cf308693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VINAYAKA\\AppData\\Local\\Temp\\ipykernel_7964\\2733628653.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['stemmed_text'] = data['tokens1'].apply(lambda tokens: [pos.stem(token) for token in tokens])\n"
     ]
    }
   ],
   "source": [
    "data['stemmed_text'] = data['tokens1'].apply(lambda tokens: [pos.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c978e7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [ , tri,  , avoid,  , sort,  , conflict]\n",
       "1     [  , friend, open,      , mental, addict,  , w...\n",
       "2     [ , save,  , girl,  , suicid, onc, .,   , go, ...\n",
       "3     [ , ca, nt, think,  , one, realli, ..., i, thi...\n",
       "4     [ , realli, one, friend,   , fit,       , cate...\n",
       "                            ...                        \n",
       "75    [  , i, 've,   , it, ,, although, i, 'm,  , ev...\n",
       "76    [  , best, friend, mom, past, away,  , od',   ...\n",
       "77    [  , camp, counselor,  , provid, stabil,  , ki...\n",
       "78    [  , girlfriend, use,   , seriou, addict, trou...\n",
       "79    [ , one, person,  , ever, talk,        , go,  ...\n",
       "Name: stemmed_text, Length: 80, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['stemmed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c349ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cd49fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VINAYAKA\\AppData\\Local\\Temp\\ipykernel_7964\\788486372.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['lemmatized_text'] = data['tokens1'].apply(lambda tokens: [wl.lemmatize(token) for token in tokens])\n"
     ]
    }
   ],
   "source": [
    "data['lemmatized_text'] = data['tokens1'].apply(lambda tokens: [wl.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0dfdb928",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmatized_text1'] = data['tokens'].apply(lambda tokens: [wl.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "745c3232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\VINAYAKA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d0654b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [ , try,  , avoid,  , sort,  , conflict]\n",
       "1     [  , friend, open,      , mental, addiction,  ...\n",
       "2     [ , saved,  , girl,  , suicide, once, .,   , g...\n",
       "3     [ , ca, nt, think,  , one, really, ..., i, thi...\n",
       "4     [ , really, one, friend,   , fit,       , cate...\n",
       "                            ...                        \n",
       "75    [  , i, 've,   , it, ,, although, i, 'm,  , ev...\n",
       "76    [  , best, friend, mom, past, away,  , od'ing,...\n",
       "77    [  , camp, counselor,  , provide, stability,  ...\n",
       "78    [  , girlfriend, used,   , serious, addiction,...\n",
       "79    [ , one, person,  , ever, talked,        , goi...\n",
       "Name: lemmatized_text, Length: 80, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemmatized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c72d98bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens1</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>lemmatized_text1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>response_1</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>try  avoid  sort  conflict</td>\n",
       "      <td>[try, avoid, sort, conflict]</td>\n",
       "      <td>[ , try,  , avoid,  , sort,  , conflict]</td>\n",
       "      <td>[ , tri,  , avoid,  , sort,  , conflict]</td>\n",
       "      <td>[ , try,  , avoid,  , sort,  , conflict]</td>\n",
       "      <td>[try, avoid, sort, conflict]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>response_2</td>\n",
       "      <td>flagged</td>\n",
       "      <td>friend open      mental addiction  weed     ...</td>\n",
       "      <td>[friend, open, mental, addiction, weed, taking...</td>\n",
       "      <td>[  , friend, open,      , mental, addiction,  ...</td>\n",
       "      <td>[  , friend, open,      , mental, addict,  , w...</td>\n",
       "      <td>[  , friend, open,      , mental, addiction,  ...</td>\n",
       "      <td>[friend, open, mental, addiction, weed, taking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>response_3</td>\n",
       "      <td>flagged</td>\n",
       "      <td>saved  girl  suicide once.   going  swallow  ...</td>\n",
       "      <td>[saved, girl, suicide, once, ., going, swallow...</td>\n",
       "      <td>[ , saved,  , girl,  , suicide, once, .,   , g...</td>\n",
       "      <td>[ , save,  , girl,  , suicid, onc, .,   , go, ...</td>\n",
       "      <td>[ , saved,  , girl,  , suicide, once, .,   , g...</td>\n",
       "      <td>[saved, girl, suicide, once, ., going, swallow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>response_4</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>cant think  one really...i think  may  indire...</td>\n",
       "      <td>[cant, think, one, really, ..., i, think, may,...</td>\n",
       "      <td>[ , ca, nt, think,  , one, really, ..., i, thi...</td>\n",
       "      <td>[ , ca, nt, think,  , one, realli, ..., i, thi...</td>\n",
       "      <td>[ , ca, nt, think,  , one, really, ..., i, thi...</td>\n",
       "      <td>[cant, think, one, really, ..., i, think, may,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>response_5</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>really one friend   fit       categories.  th...</td>\n",
       "      <td>[really, one, friend, fit, categories, ., ther...</td>\n",
       "      <td>[ , really, one, friend,   , fit,       , cate...</td>\n",
       "      <td>[ , realli, one, friend,   , fit,       , cate...</td>\n",
       "      <td>[ , really, one, friend,   , fit,       , cate...</td>\n",
       "      <td>[really, one, friend, fit, category, ., therap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response_id        class                                      response_text  \\\n",
       "0  response_1  not_flagged                         try  avoid  sort  conflict   \n",
       "1  response_2      flagged    friend open      mental addiction  weed     ...   \n",
       "2  response_3      flagged   saved  girl  suicide once.   going  swallow  ...   \n",
       "3  response_4  not_flagged   cant think  one really...i think  may  indire...   \n",
       "4  response_5  not_flagged   really one friend   fit       categories.  th...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                       [try, avoid, sort, conflict]   \n",
       "1  [friend, open, mental, addiction, weed, taking...   \n",
       "2  [saved, girl, suicide, once, ., going, swallow...   \n",
       "3  [cant, think, one, really, ..., i, think, may,...   \n",
       "4  [really, one, friend, fit, categories, ., ther...   \n",
       "\n",
       "                                             tokens1  \\\n",
       "0           [ , try,  , avoid,  , sort,  , conflict]   \n",
       "1  [  , friend, open,      , mental, addiction,  ...   \n",
       "2  [ , saved,  , girl,  , suicide, once, .,   , g...   \n",
       "3  [ , ca, nt, think,  , one, really, ..., i, thi...   \n",
       "4  [ , really, one, friend,   , fit,       , cate...   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0           [ , tri,  , avoid,  , sort,  , conflict]   \n",
       "1  [  , friend, open,      , mental, addict,  , w...   \n",
       "2  [ , save,  , girl,  , suicid, onc, .,   , go, ...   \n",
       "3  [ , ca, nt, think,  , one, realli, ..., i, thi...   \n",
       "4  [ , realli, one, friend,   , fit,       , cate...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0           [ , try,  , avoid,  , sort,  , conflict]   \n",
       "1  [  , friend, open,      , mental, addiction,  ...   \n",
       "2  [ , saved,  , girl,  , suicide, once, .,   , g...   \n",
       "3  [ , ca, nt, think,  , one, really, ..., i, thi...   \n",
       "4  [ , really, one, friend,   , fit,       , cate...   \n",
       "\n",
       "                                    lemmatized_text1  \n",
       "0                       [try, avoid, sort, conflict]  \n",
       "1  [friend, open, mental, addiction, weed, taking...  \n",
       "2  [saved, girl, suicide, once, ., going, swallow...  \n",
       "3  [cant, think, one, really, ..., i, think, may,...  \n",
       "4  [really, one, friend, fit, category, ., therap...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "49e8fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fd3e0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=data[['class','lemmatized_text1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "55af44b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>lemmatized_text1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>[try, avoid, sort, conflict]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flagged</td>\n",
       "      <td>[friend, open, mental, addiction, weed, taking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flagged</td>\n",
       "      <td>[saved, girl, suicide, once, ., going, swallow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>[cant, think, one, really, ..., i, think, may,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>[really, one, friend, fit, category, ., therap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>[i, 've, it, ,, although, i, 'm, even, i, 'd, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>flagged</td>\n",
       "      <td>[best, friend, mom, past, away, od'ing, grade, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>[camp, counselor, provide, stability, kid, lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>flagged</td>\n",
       "      <td>[girlfriend, used, serious, addiction, trouble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>[one, person, ever, talked, going, thing, ., u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          class                                   lemmatized_text1\n",
       "0   not_flagged                       [try, avoid, sort, conflict]\n",
       "1       flagged  [friend, open, mental, addiction, weed, taking...\n",
       "2       flagged  [saved, girl, suicide, once, ., going, swallow...\n",
       "3   not_flagged  [cant, think, one, really, ..., i, think, may,...\n",
       "4   not_flagged  [really, one, friend, fit, category, ., therap...\n",
       "..          ...                                                ...\n",
       "75  not_flagged  [i, 've, it, ,, although, i, 'm, even, i, 'd, ...\n",
       "76      flagged  [best, friend, mom, past, away, od'ing, grade, 5]\n",
       "77  not_flagged  [camp, counselor, provide, stability, kid, lif...\n",
       "78      flagged  [girlfriend, used, serious, addiction, trouble...\n",
       "79  not_flagged  [one, person, ever, talked, going, thing, ., u...\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "90f519ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['class'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bb42285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the lemmatized_text1 column\n",
    "count_vectors = vectorizer.fit_transform(df1['lemmatized_text1'].apply(' '.join))\n",
    "\n",
    "# Convert the count_vectors to a dataframe\n",
    "count_df = pd.DataFrame(count_vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the count_df with the original dataframe\n",
    "df2 = pd.concat([df1, count_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "106de007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>lemmatized_text1</th>\n",
       "      <th>1n</th>\n",
       "      <th>50</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>acquaintance</th>\n",
       "      <th>acted</th>\n",
       "      <th>action</th>\n",
       "      <th>activity</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>within</th>\n",
       "      <th>wood</th>\n",
       "      <th>work</th>\n",
       "      <th>worked</th>\n",
       "      <th>would</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>yearbook</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>[try, avoid, sort, conflict]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flagged</td>\n",
       "      <td>[friend, open, mental, addiction, weed, taking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flagged</td>\n",
       "      <td>[saved, girl, suicide, once, ., going, swallow...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>[cant, think, one, really, ..., i, think, may,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_flagged</td>\n",
       "      <td>[really, one, friend, fit, category, ., therap...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                   lemmatized_text1  1n  50  \\\n",
       "0  not_flagged                       [try, avoid, sort, conflict]   0   0   \n",
       "1      flagged  [friend, open, mental, addiction, weed, taking...   0   0   \n",
       "2      flagged  [saved, girl, suicide, once, ., going, swallow...   0   0   \n",
       "3  not_flagged  [cant, think, one, really, ..., i, think, may,...   0   0   \n",
       "4  not_flagged  [really, one, friend, fit, category, ., therap...   0   0   \n",
       "\n",
       "   able  absolutely  acquaintance  acted  action  activity  ...  will  within  \\\n",
       "0     0           0             0      0       0         0  ...     0       0   \n",
       "1     0           0             0      0       0         0  ...     0       0   \n",
       "2     0           0             0      0       0         0  ...     0       0   \n",
       "3     0           0             0      0       0         0  ...     0       0   \n",
       "4     0           0             0      0       0         0  ...     0       0   \n",
       "\n",
       "   wood  work  worked  would  write  year  yearbook  you  \n",
       "0     0     0       0      0      0     0         0    0  \n",
       "1     0     0       0      0      0     0         0    0  \n",
       "2     0     0       0      0      0     0         0    0  \n",
       "3     0     0       0      0      0     0         0    0  \n",
       "4     0     0       0      2      0     0         0    0  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b329501d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_flagged', 'flagged'], dtype=object)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0d81f3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['will'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "87b47960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75847349",
   "metadata": {},
   "source": [
    "# Usuage of ML classification techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "595d0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c6f4f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8125\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-Score: 0.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     flagged       0.00      0.00      0.00         2\n",
      " not_flagged       0.87      0.93      0.90        14\n",
      "\n",
      "    accuracy                           0.81        16\n",
      "   macro avg       0.43      0.46      0.45        16\n",
      "weighted avg       0.76      0.81      0.78        16\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0  2]\n",
      " [ 1 13]]\n"
     ]
    }
   ],
   "source": [
    "X = df2.drop(['class', 'lemmatized_text1'], axis=1)\n",
    "y = df2['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train the classification model (Logistic Regression)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='flagged')\n",
    "recall = recall_score(y_test, y_pred, pos_label='flagged')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='flagged')\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2e842a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VINAYAKA\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=I am very sad.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7964\\1592496875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I am very sad\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0mVector\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \"\"\"\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;31m# If input is scalar raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    762\u001b[0m                     \u001b[1;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=I am very sad.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "z=model.predict(\"I am very sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2e440790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fbf658e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VINAYAKA\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['not_flagged'], dtype=object)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b4476b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not_flagged'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8c72b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d9d3ea56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.625\n",
      "Precision: 0.16666666666666666\n",
      "Recall: 0.5\n",
      "F1-Score: 0.25\n"
     ]
    }
   ],
   "source": [
    "df2['class'] = df2['class'].map({'not_flagged': 0, 'flagged': 1})\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = df2.drop(['class', 'lemmatized_text1'], axis=1)\n",
    "y = df2['class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train the XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9dff2974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['class'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dda18e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Accuracy: 0.8125\n",
      "Precision: 0.3333333333333333\n",
      "Recall 0.5\n",
      "F1-Score: 0.4\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "}\n",
    "\n",
    "# Define the XGBoost model\n",
    "model1 = xgb.XGBClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred1 = best_model.predict(X_test)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy1 = accuracy_score(y_test, y_pred1)\n",
    "precision1 = precision_score(y_test, y_pred1)\n",
    "recall1 = recall_score(y_test, y_pred1)\n",
    "f1_1 = f1_score(y_test, y_pred1)\n",
    "\n",
    "# Print the best hyperparameters and metrics\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Precision:\", precision1)\n",
    "print(\"Recall\", recall1)\n",
    "print(\"F1-Score:\", f1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5dc92cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2e013cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 7}\n",
      "Accuracy: 0.9375\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F1-Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Define the Random Forest model\n",
    "model2 = RandomForestClassifier()\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(model2, param_distributions=param_dist, n_iter=10, scoring='accuracy', cv=3)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its predictions\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred2 = best_model.predict(X_test)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "precision2 = precision_score(y_test, y_pred2)\n",
    "recall2 = recall_score(y_test, y_pred2)\n",
    "f1_2 = f1_score(y_test, y_pred2)\n",
    "\n",
    "# Print the best hyperparameters and metrics\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Precision:\", precision2)\n",
    "print(\"Recall:\", recall2)\n",
    "print(\"F1-Score:\", f1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "bes_model=RandomForestClassifier(n_estimators=50,min_samples_split=10,min_samples_leaf=1,max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52347afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bes_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3=bes_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05854a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=joblib.dump(bes_model,'best.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1=joblib.dump(vectorizer,'vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f86208",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class'].iloc[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f01eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['response_text'].iloc[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f706d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df947fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e2a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
